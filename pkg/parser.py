from bs4 import BeautifulSoup
from typing import List, Optional, Dict
from tqdm import tqdm
from pkg.utils import _fetch_url, _json_save, _get_prices
from pkg.config import *


def _parse_links(soup: BeautifulSoup) -> List[str]:
    items = soup.find_all("a", class_="css-1tqlkj0")
    return list({
        f"https://olx{domain}{item.get('href')}"
        for item in items if item.get("href")
    })


def _parse_item_page(link: str) -> Optional[Dict[str, str]]:
    soup = _fetch_url(link)
    if not soup:
        return None

    try:
        name_tag = soup.find("h4", attrs=NAME_CLASS)
        price_tag = soup.find("h3", attrs=PRICE_CLASS)
        description_tag = soup.find("div", attrs=CONDITION_CLASS)
        seller_link_tag = soup.find("a", attrs=SELLER_LINK_SELECTOR)
        seller_name_tag = seller_link_tag.find("h4", class_=SELLER_NAME_CLASS) if seller_link_tag else None

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡ÐµÐ³Ð¾ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð½Ðµ Ñ…Ð²Ð°Ñ‚Ð°ÐµÑ‚
        missing = []
        if not name_tag:
            missing.append("Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ")
        if not description_tag:
            missing.append("ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ")
        if not seller_link_tag:
            missing.append("Ð¿Ñ€Ð¾Ð´Ð°Ð²ÐµÑ†")

        if missing:
            print(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ {', '.join(missing)}: {link}")
            return None

        return {
            "name": name_tag.text.strip() if name_tag else "ÐÐµÑ‚ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ",
            "price": price_tag.text.strip() if price_tag else "ÐÐµÑ‚ Ñ†ÐµÐ½Ñ‹",
            "description": description_tag.text.strip(),
            "seller_name": seller_name_tag.text.strip() if seller_name_tag else None,
            "seller_link": f"https://www.olx.kz{seller_link_tag['href']}" if seller_link_tag and seller_link_tag.has_attr(
                "href") else None,
            "link": link
        }

    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð·Ð±Ð¾Ñ€Ðµ {link}: {e}")
        return None


def parse_items(soup: BeautifulSoup) -> None:
    if not soup:
        print("âŒ ÐÐµÑ‚ HTML Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸.")
        return

    links = _parse_links(soup)
    print(f"ðŸ”— ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÑÑ‹Ð»Ð¾Ðº: {len(links)}")

    all_data = []

    for link in tqdm(links, desc="ðŸ“¦ ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²"):
        item = _parse_item_page(link)
        if item:
            all_data.append(item)
        else:
            print(f"âš ï¸ ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾: {link}")

    print(f"âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾: {len(all_data)}")
    _json_save(all_data)


def average_price(soup: BeautifulSoup) -> int:
    if not soup:
        return 0

    price_list = _get_prices(soup)
    return int(sum(price_list) / len(price_list)) if price_list else 0


def get_page(query: str, country: str = 'KZ', currency: str = 'KZT', condition: str = 'all') -> Optional[
    BeautifulSoup]:
    from urllib.parse import quote_plus

    if not query:
        raise ValueError("The 'query' parameter is required and cannot be empty.")

    try:
        url = f"https://www.olx{countryDomain[country]}/list/astana/q-{quote_plus(query)}/{currencyDict[currency]}{conditionDict[condition]}"
    except KeyError as e:
        print(f"Invalid value of {e}\nUse 'help' function to see available parameters")
        return None

    global domain
    domain = countryDomain[country]

    return _fetch_url(url)
