from urllib.parse import quote_plus

from bs4 import BeautifulSoup
from typing import List, Optional, Dict
from tqdm import tqdm
from pkg.utils import _fetch_url, _json_save, _get_prices
from pkg.config import *


def _parse_links(soup: BeautifulSoup) -> List[str]:
    items = soup.find_all("a", class_="css-1tqlkj0")
    return list({
        f"https://olx{domain}{item.get('href')}"
        for item in items if item.get("href")
    })


def _parse_item_page(link: str) -> Optional[Dict[str, str]]:
    soup = _fetch_url(link)
    if not soup:
        return None

    try:
        name_tag = soup.find("h4", attrs=NAME_CLASS)
        price_tag = soup.find("h3", attrs=PRICE_CLASS)
        description_tag = soup.find("div", attrs=CONDITION_CLASS)
        seller_link_tag = soup.find("a", attrs=SELLER_LINK_SELECTOR)
        seller_name_tag = seller_link_tag.find("h4", class_=SELLER_NAME_CLASS) if seller_link_tag else None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á–µ–≥–æ –∏–º–µ–Ω–Ω–æ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç
        missing = []
        if not name_tag:
            missing.append("–Ω–∞–∑–≤–∞–Ω–∏–µ")
        if not description_tag:
            missing.append("—Å–æ—Å—Ç–æ—è–Ω–∏–µ")
        if not seller_link_tag:
            missing.append("–ø—Ä–æ–¥–∞–≤–µ—Ü")

        if missing:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å {', '.join(missing)}: {link}")
            return None

        return {
            "name": name_tag.text.strip() if name_tag else "–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è",
            "price": price_tag.text.strip() if price_tag else "–ù–µ—Ç —Ü–µ–Ω—ã",
            "description": description_tag.text.strip(),
            "seller_name": seller_name_tag.text.strip() if seller_name_tag else None,
            "seller_link": f"https://www.olx.kz{seller_link_tag['href']}" if seller_link_tag and seller_link_tag.has_attr(
                "href") else None,
            "link": link
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ {link}: {e}")
        return None


def parse_items(soup: BeautifulSoup) -> None:
    if not soup:
        print("‚ùå –ù–µ—Ç HTML –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    links = _parse_links(soup)
    print(f"üîó –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫: {len(links)}")

    all_data = []

    for link in tqdm(links, desc="üì¶ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤"):
        item = _parse_item_page(link)
        if item:
            all_data.append(item)
        else:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {link}")

    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(all_data)}")
    _json_save(all_data)


def average_price(soup: BeautifulSoup) -> int:
    if not soup:
        return 0

    price_list = _get_prices(soup)
    return int(sum(price_list) / len(price_list)) if price_list else 0


def get_page(query: str, country: str = 'KZ', currency: str = 'KZT', condition: str = 'all', city: str = 'astana') -> \
        Optional[
            BeautifulSoup]:
    from urllib.parse import quote_plus

    if not query:
        raise ValueError("The 'query' parameter is required and cannot be empty.")

    try:
        url = f"https://www.olx{countryDomain[country]}/list/{city}/q-{quote_plus(query)}/{currencyDict[currency]}{conditionDict[condition]}"
    except KeyError as e:
        print(f"Invalid value of {e}\nUse 'help' function to see available parameters")
        return None

    global domain
    domain = countryDomain[country]

    return _fetch_url(url)


def get_all_pages(query: str, country: str = 'KZ', currency: str = 'KZT',
                  condition: str = 'all', city: str = '') -> List[BeautifulSoup]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã OLX –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É.

    Parameters
    ----------
    query : str
        –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä: iphone).
    country : str
        –°—Ç—Ä–∞–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 'KZ').
    currency : str
        –í–∞–ª—é—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 'KZT').
    condition : str
        –°–æ—Å—Ç–æ—è–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: 'all', 'used', 'new'
    city : str
        –ù–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ (–ø–æ –∂–µ–ª–∞–Ω–∏—é).

    Returns
    -------
    List[BeautifulSoup]
        –°–ø–∏—Å–æ–∫ HTML —Å—Ç—Ä–∞–Ω–∏—Ü.
    """

    pages = []
    page_number = 1
    global domain
    domain = countryDomain[country]

    while page_number <= 3:
        try:
            base_path = f"/list/{city}/" if city else "/list/"
            url = f"https://www.olx{domain}{base_path}q-{quote_plus(query)}/?page={page_number}{currencyDict[currency]}{conditionDict[condition]}"
            print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_number} ‚Üí {url}")

            soup = _fetch_url(url)
            if not soup:
                print("‚ö†Ô∏è –°—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å.")
                break

            items = soup.find_all("div", {"data-cy": "l-card"})
            if not items:
                print("üîö –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ—Ç.")
                break

            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
            pages.append(soup)
            page_number += 1

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}: {e}")
            break

    return pages
